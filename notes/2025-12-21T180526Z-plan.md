# Plan: Modularize VM tests and add RAID/LVM cleanup coverage

## Context
- `tests/test_boot_image_vm.py` is ~2.7k lines mixing fixtures, helpers, controller logic, and scenarios.
- VM-based cleanup regression (Task 2) needs a reliable, runnable flow without hangs/timeouts.
- Prior runs hung/failed; need recorded, stepwise progress to avoid losing work.

## Goals
- Break the monolithic VM test into focused modules without behavior changes.
- Implement the RAID/LVM residue regression test (Task 2) using the new structure.
- Keep VM runs tractable and reproducible; record every attempt and outcome.

## Phased plan

1) **Inventory and define module boundaries**
   - Catalogue fixtures and helpers currently in `tests/test_boot_image_vm.py` (tools/ISO/disks fixtures, SSH key handling, metadata writers, `BootImageVM` controller, command utilities, test cases).
   - Proposed modules:
     - `tests/vm/fixtures.py` – tool checks, ISO build, disk image fixtures, SSH key generation.
     - `tests/vm/metadata.py` – metadata/diagnostic helpers.
     - `tests/vm/controller.py` – `BootImageVM` class and interaction helpers.
     - `tests/vm/cleanup_plan.py` – RAID/LVM cleanup utilities shared by scenarios.
     - `tests/vm/test_pre_nixos_vm.py` – existing scenarios (split out from monolith).
     - `tests/vm/test_pre_nixos_cleanup.py` – new RAID/LVM residue regression scenario.
   - Maintain fixture names/scopes for compatibility during the split.

2) **Mechanical split with compatibility layer**
   - Move helpers/dataclasses into new modules; adjust imports in-place.
   - Optionally keep a thin shim (`tests/test_boot_image_vm.py` or `tests/vm/conftest.py`) re-exporting fixtures to minimize churn.

3) **Add RAID/LVM cleanup regression test (Task 2)**
   - Preseed mdadm+LVM residue inside the VM (simulate `/dev/md127` leftovers).
   - Run `pre-nixos` end-to-end; assert cleanup succeeds and post-run layout matches plan.
   - Capture artifacts on failure: dmesg, pre-nixos logs, disko output.

4) **Runtime management, measurements, and reliability**
   - Capture and report three timings on every run: (a) build time for the boot image (including squashfs or alternative image build), (b) VM boot time from start to SSH-ready, and (c) end-to-end test duration. Record each in the run ledger with UTC timestamps to see whether the 1h session limit is dominated by build, boot, or test steps.
   - Prefer fewer VM boots per session (1–2 end-to-end runs) to avoid exceeding the observed ~1h session ceiling; queue additional runs for a new session if projected wall-clock exceeds 45–50 minutes.
   - Surface timeout env vars (`BOOT_IMAGE_VM_SPAWN_TIMEOUT`, `BOOT_IMAGE_VM_LOGIN_TIMEOUT`) but treat them as diagnostics: default to generous values (e.g., 900s/300s) and only lower when measurements show hangs rather than slow progress.
   - Add markers (`@pytest.mark.vm`, `@pytest.mark.slow`) and document selective invocation; do not silently skip—treat missing tools/timeouts as failures and record them.
   - Reuse built images across scenarios; keep disk sizes minimal while safe.

5) **Documentation**
   - Add `tests/vm/README.md` explaining module layout, required tools, env vars, and how to run VM tests.
   - Keep run logs/attempts in `notes/` with UTC timestamps; include commands and outcomes to avoid losing progress.

6) **Iterative execution loop**
   - After splitting, run existing VM test + new cleanup test; collect outputs.
   - Apply half-splitting on failures; iterate fixes with logs per run.
   - Once stable, remove legacy monolith wrapper if superseded by new modules.

## Immediate next steps
- Extract fixtures/controller/metadata into `tests/vm/` modules per above.
- Add README outlining run instructions and markers.
- Implement cleanup regression test using shared helpers.
- Execute VM tests, record results in `notes/` with timestamps; iterate until green.

## Detailed breakdown and guardrails
- **Inventory method**: use `rg "@pytest.fixture" tests/test_boot_image_vm.py` and skim surrounding helpers to map dependencies. Summarize scopes/args in a table before moving code.
- **Module creation**: start with `tests/vm/__init__.py` plus empty modules carrying docstrings that enumerate the contents to be migrated. Keep import-only `tests/test_boot_image_vm.py` shim that re-exports fixtures/tests until everything runs.
- **Dependency hygiene**: avoid new runtime deps; reuse existing `subprocess`/`pathlib` tooling. Keep helper functions pure where possible.
- **Timeout surfacing**: default to generous diagnostics (`BOOT_IMAGE_VM_SPAWN_TIMEOUT=900`, `BOOT_IMAGE_VM_LOGIN_TIMEOUT=300`) while real performance relies on measured timings; document overrides in README and thread them through helper constructors.
- **RAID/LVM residue recipe (draft)**: inside the VM, as root:
  1. `mdadm --create /dev/md127 --force --raid1 --level=1 --metadata=1.2 --raid-devices=2 /dev/vdb /dev/vdc`
  2. `pvcreate /dev/md127 && vgcreate vg_residue /dev/md127 && lvcreate -n lv_residue -l 100%FREE vg_residue`
  3. Write a sentinel file to `/dev/vg_residue/lv_residue` to verify teardown.
  4. Skip wiping on purpose; expect pre-nixos cleanup to remove mdadm+LVM remnants and leave block devices unused.
- **Logging discipline**: every VM run should capture console log, `dmesg`, and `/tmp/pre-nixos*.log` into a run-specific folder under `notes/`. Include command-line, environment overrides, and elapsed time.
- **Exit criteria**: monolithic file replaced by `tests/vm/*` modules; RAID/LVM regression green; README documents selective invocation and failure triage.

## Work log stubs (fill as we execute)
- [x] **Inventory snapshot** – captured fixture/helper table and target modules in `notes/2025-12-21T220149Z-vm-split.md`.
- [x] **Module skeletons** – create empty `tests/vm/*.py` files with docstrings describing intended contents; wire `__init__.py` if needed and keep imports passing lint. Added `tests/vm/README.md` to document layout and runtime expectations.
- [x] **Shim plan** – using `tests/vm/conftest.py` as a re-export shim while helpers migrate out of `tests/test_boot_image_vm.py`.
- [x] **RAID/LVM recipe** – codified the command sequence and teardown expectations in `tests/vm/cleanup_plan.py` and mirrored them in the notes/README.
- [ ] **Timing discipline** – for every run, capture: boot-image build duration (noting whether squashfs was built), VM boot-to-SSH window, and whole-test elapsed wall-clock; add UTC timestamps and whether the session hit the ~1h ceiling.
- [x] **Timeout defaults** – codify generous default values for spawn/login timeouts (diagnostic only) and note how to override via env vars; thread through helper constructors.
- [ ] **Run ledger** – capture each VM test attempt with UTC timestamp, command, result, and any artifacts collected; store paths in notes entry.

## Fast-path boot image for tests (squashfs avoidance idea)
- Hypothesis: a large share of boot-image build time is spent generating squashfs. For test-only runs, explore building an alternative image (e.g., raw ext4 or qcow2 without squashfs) that the VM can boot more quickly. Guardrails:
  - Keep the production squashfs path intact; the fast-path is opt-in via a flag/env var in the test harness.
  - Measure build times with and without squashfs to validate the benefit; record both in the run ledger alongside the per-run build/boot/test timings.
  - Verify parity: the fast-path image must exercise the same pre-nixos logic and cleanup steps even if the filesystem layout differs.
  - Document selection knobs in `tests/vm/README.md` alongside the timing expectations.
